{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98c30e1",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction & Revenue at Risk Analysis\n",
    "\n",
    "**Course:** Quantitative Risk Management — Final Project  \n",
    "**Author:** Eduard Aguado Marin  \n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook builds a machine learning pipeline to predict customer churn in a telecom company and quantify the **Revenue at Risk (RAR)** — the annual revenue exposed to loss from customers likely to leave.\n",
    "\n",
    "**Pipeline overview:**\n",
    "1. **Data Cleaning & Feature Engineering** — parse European-format monetary columns, bucketize tenure, encode categoricals\n",
    "2. **Exploratory Data Analysis** — distributions, correlations, PCA dimensionality reduction, K-Means clustering\n",
    "3. **Modelling** — Logistic Regression (baseline), Random Forest, XGBoost with GridSearchCV; SMOTE oversampling to handle class imbalance\n",
    "4. **Model Interpretation** — SHAP values, permutation importance, ROC curves\n",
    "5. **Revenue at Risk** — translate churn probabilities into dollar-denominated business risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36549883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, StratifiedKFold\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Plotting defaults\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a2ea2",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Data Loading & Initial Inspection\n",
    "\n",
    "The dataset contains **7,043 telecom customer records** with 23 columns covering demographics, service subscriptions, billing, support tickets, and the binary churn label.\n",
    "\n",
    "> **Note:** The CSV uses a semicolon separator and European decimal format (commas instead of dots), so `MonthlyCharges` and `TotalCharges` are initially read as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('telcom_dataset.csv', sep=';')\n",
    "df.replace('', np.nan, inplace=True)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df.isna().sum()[df.isna().sum() > 0]}\"\n",
    "      if df.isna().any().any() else \"No missing values found.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7459122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values per column — helps decide encoding strategy\n",
    "# Binary (2 unique) → 0/1, Few categories (3-4) → one-hot, Continuous → keep as-is\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72139ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MonthlyCharges and TotalCharges are stored as object (string) due to European comma decimals\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution — 26.5% churn rate indicates moderate class imbalance\n",
    "df.Churn.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46241809",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Data Cleaning & Feature Engineering\n",
    "\n",
    "Steps:\n",
    "1. Drop `customerID` (not predictive)\n",
    "2. Parse `MonthlyCharges` / `TotalCharges` from European string format → float\n",
    "3. Bucketize `tenure` into 7 interpretable groups (0-6 mo, 6-12 mo, ..., 60+ mo)\n",
    "4. Encode binary columns (Yes/No → 1/0) and one-hot encode multi-category columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop customerID — it's a unique identifier, not a feature\n",
    "if 'customerID' in df.columns:\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "def clean_monetary_columns(df, cols=('TotalCharges', 'MonthlyCharges')):\n",
    "    \"\"\"Convert European-format monetary strings to float.\n",
    "    \n",
    "    Handles: '1.234,56' (dot thousands, comma decimal), '12,5' (comma decimal only),\n",
    "    empty strings, and NaN values.\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        s = df[col].astype(str).str.strip()\n",
    "        s = s.mask((s == '') | (s.str.lower() == 'nan'), '0')\n",
    "\n",
    "        has_comma = s.str.contains(',', regex=False)\n",
    "        has_dot = s.str.contains('.', regex=False)\n",
    "        s2 = s.copy()\n",
    "        mask_both = has_comma & has_dot\n",
    "\n",
    "        # '1.234,56' → remove dot thousands separator, then comma → dot\n",
    "        if mask_both.any():\n",
    "            s2.loc[mask_both] = (\n",
    "                s.loc[mask_both]\n",
    "                .str.replace('.', '', regex=False)\n",
    "                .str.replace(',', '.', regex=False)\n",
    "            )\n",
    "\n",
    "        # '12,5' → '12.5'\n",
    "        only_comma = has_comma & ~mask_both\n",
    "        if only_comma.any():\n",
    "            s2.loc[only_comma] = s.loc[only_comma].str.replace(',', '.', regex=False)\n",
    "\n",
    "        s2 = s2.str.replace(r'[^0-9\\.\\-]', '', regex=True)\n",
    "        df[col] = pd.to_numeric(s2, errors='coerce').fillna(0.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = clean_monetary_columns(df)\n",
    "print(f\"MonthlyCharges dtype: {df['MonthlyCharges'].dtype}, range: [{df['MonthlyCharges'].min():.2f}, {df['MonthlyCharges'].max():.2f}]\")\n",
    "print(f\"TotalCharges   dtype: {df['TotalCharges'].dtype},   range: [{df['TotalCharges'].min():.2f}, {df['TotalCharges'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketize tenure into interpretable groups\n",
    "# Raw tenure has 73 unique values (months) — grouping reduces noise and aids interpretation\n",
    "TENURE_LABELS = {1: '0–6 mo', 2: '6–12 mo', 3: '12–24 mo',\n",
    "                 4: '24–36 mo', 5: '36–48 mo', 6: '48–60 mo', 7: '60+ mo'}\n",
    "\n",
    "def bucketize_tenure(t):\n",
    "    if t <= 6:   return 1\n",
    "    if t <= 12:  return 2\n",
    "    if t <= 24:  return 3\n",
    "    if t <= 36:  return 4\n",
    "    if t <= 48:  return 5\n",
    "    if t <= 60:  return 6\n",
    "    return 7\n",
    "\n",
    "df['tenure_group'] = df['tenure'].apply(bucketize_tenure)\n",
    "df.drop('tenure', axis=1, inplace=True)\n",
    "\n",
    "print(\"Tenure group distribution:\")\n",
    "print(df['tenure_group'].value_counts().sort_index().rename(TENURE_LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed941c",
   "metadata": {},
   "source": [
    "### 2.1 Feature Encoding\n",
    "\n",
    "Encoding strategy:\n",
    "- **Binary Yes/No columns** → 0/1 (including those with \"No internet service\" / \"No phone service\" mapped to 0)\n",
    "- **Gender** → binary `male` column (Female=0, Male=1)\n",
    "- **Multi-category columns** (`InternetService`, `Contract`, `PaymentMethod`) → one-hot encoding with `drop_first=True` to avoid multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender → binary\n",
    "df['male'] = df['gender'].map({'Female': 0, 'Male': 1})\n",
    "df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "# Yes/No columns → 0/1\n",
    "for col in ['Partner', 'Dependents', 'PaperlessBilling', 'Churn', 'PhoneService']:\n",
    "    df[col] = df[col].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Columns with \"No internet service\" → treat as 0 (service not used)\n",
    "for col in ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "            'TechSupport', 'StreamingTV', 'StreamingMovies']:\n",
    "    df[col] = df[col].map({'No internet service': 0, 'No': 0, 'Yes': 1})\n",
    "\n",
    "# MultipleLines: \"No phone service\" → 0\n",
    "df['MultipleLines'] = df['MultipleLines'].map({'No phone service': 0, 'No': 0, 'Yes': 1})\n",
    "\n",
    "# One-hot encode multi-category features (drop_first avoids perfect multicollinearity)\n",
    "df = pd.get_dummies(df, columns=['InternetService', 'Contract', 'PaymentMethod'], drop_first=True)\n",
    "df = df.astype({col: int for col in df.select_dtypes('bool').columns})\n",
    "\n",
    "# Sanitize column names (replace spaces with underscores)\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final class distribution after encoding\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "print(f\"No Churn: {churn_counts[0]} ({churn_counts[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Churn:    {churn_counts[1]} ({churn_counts[1]/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTotal features: {df.shape[1] - 1} + 1 target = {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833c93a",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Exploratory Data Analysis\n",
    "\n",
    "We examine the data from three angles:\n",
    "1. **Univariate distributions** — understand the shape of each feature\n",
    "2. **Feature correlations** — identify multicollinearity and features associated with churn\n",
    "3. **PCA + K-Means clustering** — discover natural customer segments and their churn concentration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c496e",
   "metadata": {},
   "source": [
    "### 3.1 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Helper: plot distributions for a set of columns ──\n",
    "def plot_variable_distributions(df, title_prefix=''):\n",
    "    num_vars = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    cat_vars = df.select_dtypes(include=['object', 'category']).columns\n",
    "    total_vars = len(num_vars) + len(cat_vars)\n",
    "    cols = 3\n",
    "    rows = (total_vars + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(num_vars):\n",
    "        if set(df[col].dropna().unique()).issubset({0, 1}):\n",
    "            sns.countplot(x=df[col], ax=axes[i])\n",
    "            axes[i].set_xticks([0, 1])\n",
    "        else:\n",
    "            sns.histplot(df[col], bins=20, kde=False, ax=axes[i])\n",
    "        axes[i].set_title(f'{title_prefix}{col}')\n",
    "\n",
    "    for j, col in enumerate(cat_vars, start=len(num_vars)):\n",
    "        sns.countplot(x=df[col], ax=axes[j])\n",
    "        axes[j].set_title(f'{title_prefix}{col}')\n",
    "        axes[j].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for k in range(total_vars, len(axes)):\n",
    "        fig.delaxes(axes[k])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn vs Non-Churn bar plot with counts and percentages\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "churn_percent = df['Churn'].value_counts(normalize=True) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.barplot(x=churn_counts.index, y=churn_counts.values, palette='viridis', ax=ax)\n",
    "for i, (count, pct) in enumerate(zip(churn_counts.values, churn_percent.values)):\n",
    "    ax.text(i, count + 50, f'{count} ({pct:.1f}%)', ha='center', va='bottom', fontsize=12)\n",
    "ax.set_xticklabels(['No Churn', 'Churn'])\n",
    "ax.set_ylabel('Number of Customers')\n",
    "ax.set_title('Customer Churn Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b83c7",
   "metadata": {},
   "source": [
    "### 3.2 Feature Distributions\n",
    "\n",
    "We split features into **binary** (0/1) and **non-binary** groups for cleaner visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary features (0/1)\n",
    "binary_vars = [col for col in df.columns if set(df[col].dropna().unique()).issubset({0, 1})]\n",
    "plot_variable_distributions(df[binary_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55796553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-binary features (continuous and multi-category)\n",
    "other_vars = [col for col in df.columns if col not in binary_vars]\n",
    "plot_variable_distributions(df[other_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b1877",
   "metadata": {},
   "source": [
    "### 3.3 Feature Correlations\n",
    "\n",
    "The heatmap below highlights which features are most correlated with `Churn` and with each other. Strong inter-feature correlations (e.g., `MonthlyCharges` ↔ streaming services) may affect logistic regression coefficients but are handled well by tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d825218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 11))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, fmt='.1f', cmap='coolwarm', square=True,\n",
    "            linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d00f8a",
   "metadata": {},
   "source": [
    "### 3.4 PCA & K-Means Clustering\n",
    "\n",
    "**Goal:** Identify natural customer segments and check whether churn concentrates in specific clusters.\n",
    "\n",
    "**Approach:**\n",
    "- Standardize features, then apply PCA retaining 95% of variance\n",
    "- Use the **elbow method** (WCSS) and **silhouette scores** to evaluate cluster quality\n",
    "- Visualize clusters in the first two principal components, split by churn status\n",
    "\n",
    "> **Note on k selection:** The silhouette score is maximized at k=2, but we choose **k=3** because it produces more interpretable segments — the 3-cluster solution clearly separates a high-churn group (Cluster 0) from two lower-churn groups, whereas k=2 merges these distinct behavioral profiles into a single heterogeneous cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── PCA ──────────────────────────────────────────────────────────────────────\n",
    "X = df.drop(columns='Churn')\n",
    "y = df['Churn']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca_full = PCA(n_components=0.95)\n",
    "X_pca = pca_full.fit_transform(X_scaled)\n",
    "print(f\"PCA retained {X_pca.shape[1]} components, explaining \"\n",
    "      f\"{pca_full.explained_variance_ratio_.sum():.1%} of variance\")\n",
    "\n",
    "# ── Evaluate k with silhouette scores ────────────────────────────────────────\n",
    "k_range = range(2, 11)\n",
    "sil_scores = []\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_pca)\n",
    "    sil_scores.append(silhouette_score(X_pca, labels))\n",
    "\n",
    "print(\"\\nSilhouette scores:\", dict(zip(k_range, [round(s, 3) for s in sil_scores])))\n",
    "print(f\"Best k by silhouette: {list(k_range)[sil_scores.index(max(sil_scores))]}\")\n",
    "\n",
    "# We override the silhouette-optimal k with k=3 for better interpretability\n",
    "# (see markdown note above — k=3 separates the high-churn group more clearly)\n",
    "CHOSEN_K = 3\n",
    "print(f\"Selected k = {CHOSEN_K}\")\n",
    "\n",
    "# ── Elbow method (WCSS) ─────────────────────────────────────────────────────\n",
    "k_values = range(1, 16)\n",
    "wcss = []\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "    km.fit(X_pca)\n",
    "    wcss.append(km.inertia_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(list(k_values), wcss, marker='o', linewidth=2)\n",
    "axes[0].axvline(CHOSEN_K, color='red', linestyle='--', alpha=0.7, label=f'Chosen k={CHOSEN_K}')\n",
    "axes[0].set_xlabel('Number of clusters k')\n",
    "axes[0].set_ylabel('WCSS (inertia)')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].set_xticks(list(k_values))\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(list(k_range), sil_scores, marker='s', linewidth=2, color='orange')\n",
    "axes[1].axvline(CHOSEN_K, color='red', linestyle='--', alpha=0.7, label=f'Chosen k={CHOSEN_K}')\n",
    "axes[1].set_xlabel('Number of clusters k')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis')\n",
    "axes[1].set_xticks(list(k_range))\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcde6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Fit K-Means with k=3 and visualize ───────────────────────────────────────\n",
    "kmeans = KMeans(n_clusters=CHOSEN_K, random_state=42, n_init=20)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "X_plot = X_pca[:, :2]\n",
    "plot_df = pd.DataFrame(X_plot, columns=['PC1', 'PC2']).assign(cluster=clusters, churn=y.values)\n",
    "\n",
    "# Side-by-side scatter plots: Non-Churned vs Churned\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.scatterplot(data=plot_df[plot_df['churn'] == 0], x='PC1', y='PC2', hue='cluster',\n",
    "                s=50, alpha=0.8, edgecolor='k', linewidth=0.3, ax=axes[0])\n",
    "axes[0].set_title('Non-Churned Customers')\n",
    "\n",
    "sns.scatterplot(data=plot_df[plot_df['churn'] == 1], x='PC1', y='PC2', hue='cluster',\n",
    "                s=50, alpha=0.8, edgecolor='k', linewidth=0.3, ax=axes[1])\n",
    "axes[1].set_title('Churned Customers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Churn distribution per cluster\n",
    "print(\"\\nCluster sizes and churn rates:\")\n",
    "ct = pd.crosstab(plot_df['cluster'], plot_df['churn'], margins=True)\n",
    "ct.columns = ['No Churn', 'Churn', 'Total']\n",
    "ct['Churn Rate'] = (ct['Churn'] / ct['Total'] * 100).round(1).astype(str) + '%'\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57432ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA loadings — which original features contribute most to PC1 and PC2\n",
    "pc_df = pd.DataFrame(\n",
    "    pca_full.components_[:2].T,\n",
    "    index=X.columns,\n",
    "    columns=['PC1', 'PC2']\n",
    ").sort_values('PC1', ascending=False)\n",
    "\n",
    "print(\"Top PC1 drivers: MonthlyCharges, TotalCharges, StreamingTV/Movies (service engagement)\")\n",
    "print(\"Top PC2 drivers: Contract_Two_year, tenure_group (customer loyalty) vs Electronic check, Fiber optic (churn risk)\")\n",
    "pc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211de28",
   "metadata": {},
   "source": [
    "**Key finding:** ~75% of all churned customers are concentrated in **Cluster 0** (1,272 out of 1,869). This cluster has a ~49.5% churn rate, compared to ~15-18% in the other clusters. Feature importance analysis in the modelling section will help explain *why* these customers churn — likely driven by short tenure, fiber optic internet, month-to-month contracts, and high tech ticket counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afeef71",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Modelling\n",
    "\n",
    "We train three classifiers of increasing complexity:\n",
    "\n",
    "| Model | Purpose |\n",
    "|-------|---------|\n",
    "| **Logistic Regression** | Interpretable linear baseline |\n",
    "| **Random Forest** | Non-linear ensemble with built-in feature importance |\n",
    "| **XGBoost** | Gradient-boosted trees, hyperparameter-tuned via GridSearchCV |\n",
    "\n",
    "**Class imbalance handling:**\n",
    "- **SMOTE** (Synthetic Minority Over-sampling Technique) is applied to the **training set only** to avoid data leakage into the test set\n",
    "- Random Forest also uses `class_weight={0: 1, 1: 2}` as additional imbalance correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2fae6",
   "metadata": {},
   "source": [
    "### 4.1 Train/Test Split & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Train/test split (80/20, stratified on Churn) ────────────────────────────\n",
    "feature_cols = df.columns[df.columns != 'Churn']\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Churn'])\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "Y_train = train['Churn']\n",
    "X_test  = test[feature_cols]\n",
    "Y_test  = test['Churn']\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} samples | Test: {X_test.shape[0]} samples\")\n",
    "print(f\"Train churn rate: {Y_train.mean():.1%} | Test churn rate: {Y_test.mean():.1%}\")\n",
    "\n",
    "# ── Helper functions ─────────────────────────────────────────────────────────\n",
    "def print_metrics(model, X_train, Y_train, X_test, Y_test, label=\"Model\"):\n",
    "    \"\"\"Print classification reports for train and test sets.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {label} — Classification Report\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nTraining Set:\")\n",
    "    print(classification_report(Y_train, model.predict(X_train)))\n",
    "    print(\"Test Set:\")\n",
    "    print(classification_report(Y_test, model.predict(X_test)))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot a heatmap confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1de60",
   "metadata": {},
   "source": [
    "### 4.2 SMOTE Oversampling\n",
    "\n",
    "SMOTE generates synthetic minority-class samples by interpolating between existing minority neighbors. It is applied **only to the training set** — the test set retains the original class distribution to give realistic performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba41bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before SMOTE:\", Counter(Y_train))\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "print(\"After  SMOTE:\", Counter(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2529b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class balance after SMOTE (training) vs original (test)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, data, title in [(axes[0], Y_train, 'Training Set (After SMOTE)'),\n",
    "                         (axes[1], Y_test, 'Test Set (Original Distribution)')]:\n",
    "    counts = data.value_counts().sort_index()\n",
    "    pcts = (counts / counts.sum() * 100)\n",
    "    sns.barplot(x=counts.index, y=counts.values, palette='viridis', ax=ax)\n",
    "    for i, (cnt, pct) in enumerate(zip(counts.values, pcts.values)):\n",
    "        ax.text(i, cnt + cnt * 0.02, f'{cnt} ({pct:.1f}%)', ha='center', fontsize=11)\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['No Churn', 'Churn'])\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891b481",
   "metadata": {},
   "source": [
    "### 4.3 Model 1: Logistic Regression (Baseline)\n",
    "\n",
    "A simple linear model that serves as our baseline. Since SMOTE already balances the training data, we use default settings with `max_iter=1000` to ensure convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "print_metrics(lr, X_train, Y_train, X_test, Y_test, label=\"Logistic Regression\")\n",
    "plot_confusion_matrix(Y_test, lr.predict(X_test), title=\"Logistic Regression — Confusion Matrix (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c32b1",
   "metadata": {},
   "source": [
    "### 4.4 Model 2: Random Forest\n",
    "\n",
    "Key hyperparameters:\n",
    "- `n_estimators=1000` — large ensemble for stable predictions\n",
    "- `criterion='entropy'` — information gain split criterion\n",
    "- `max_depth=10` — limits tree depth to reduce overfitting\n",
    "- `class_weight={0: 1, 1: 2}` — additional penalty for misclassifying churned customers (on top of SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=1000, criterion='entropy', max_depth=10,\n",
    "    max_features='sqrt', min_samples_split=8,\n",
    "    class_weight={0: 1, 1: 2}, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "print_metrics(rf, X_train, Y_train, X_test, Y_test, label=\"Random Forest\")\n",
    "plot_confusion_matrix(Y_test, rf.predict(X_test), title=\"Random Forest — Confusion Matrix (Test Set)\")\n",
    "\n",
    "# Gini-based feature importance\n",
    "importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features (Gini importance):\")\n",
    "print(importances.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b36dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance — more reliable than Gini for correlated features\n",
    "# Measures actual accuracy drop when each feature is randomly shuffled\n",
    "perm_result = permutation_importance(rf, X_test, Y_test, n_repeats=30, random_state=42, n_jobs=-1)\n",
    "perm_importances = pd.Series(perm_result.importances_mean, index=X_test.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features (permutation importance):\")\n",
    "print(perm_importances.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cdfd7b",
   "metadata": {},
   "source": [
    "### 4.5 Model 3: XGBoost (with GridSearchCV)\n",
    "\n",
    "XGBoost is tuned via exhaustive grid search over key hyperparameters, using **5-fold stratified cross-validation** and **ROC-AUC** as the scoring metric (better suited for imbalanced targets than accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators':      [200, 500, 1000],\n",
    "    'max_depth':         [3, 6, 10],\n",
    "    'learning_rate':     [0.01, 0.1, 0.2],\n",
    "    'subsample':         [0.7, 1.0],\n",
    "    'colsample_bytree':  [0.7, 1.0],\n",
    "    'scale_pos_weight':  [1, 2],\n",
    "}\n",
    "\n",
    "gs_xgb = GridSearchCV(\n",
    "    xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss'),\n",
    "    param_grid_xgb,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "gs_xgb.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"\\nBest CV ROC-AUC: {gs_xgb.best_score_:.4f}\")\n",
    "print(f\"Best parameters: {gs_xgb.best_params_}\")\n",
    "\n",
    "xgb_model = gs_xgb.best_estimator_\n",
    "print_metrics(xgb_model, X_train, Y_train, X_test, Y_test, label=\"XGBoost\")\n",
    "plot_confusion_matrix(Y_test, xgb_model.predict(X_test), title=\"XGBoost — Confusion Matrix (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a07b3",
   "metadata": {},
   "source": [
    "### 4.6 Model Comparison — ROC Curves\n",
    "\n",
    "The ROC curve plots the True Positive Rate vs False Positive Rate at various classification thresholds. A higher AUC (Area Under Curve) indicates better discrimination between churned and non-churned customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": lr,\n",
    "    \"Random Forest\": rf,\n",
    "    \"XGBoost\": xgb_model\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for ax, (label, model) in zip(axes, models.items()):\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(Y_test, y_proba)\n",
    "    auc = roc_auc_score(Y_test, y_proba)\n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_title(f'ROC Curve — {label}')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f4a2c",
   "metadata": {},
   "source": [
    "### 4.7 SHAP Values (Model Interpretability)\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) assigns each feature a contribution to each individual prediction:\n",
    "- **Bar plot** — global feature importance (mean |SHAP| across all test samples)\n",
    "- **Beeswarm plot** — shows both importance *and* direction of effect (red = high feature value, blue = low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29df1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values for the Random Forest model\n",
    "explainer = shap.Explainer(rf)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Global feature importance\n",
    "shap.plots.bar(shap_values, max_display=15, show=False)\n",
    "plt.title('SHAP — Global Feature Importance (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Beeswarm: direction + magnitude of feature effects\n",
    "shap.plots.beeswarm(shap_values, max_display=10, show=False)\n",
    "plt.title('SHAP — Beeswarm Plot (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b606b",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Revenue at Risk (RAR) Analysis\n",
    "\n",
    "The final step translates model predictions into **business value**. We use the Random Forest model to estimate each customer's churn probability, then calculate:\n",
    "\n",
    "- **Revenue at Risk** = sum of annualized charges for *currently non-churned* customers whose predicted churn probability exceeds 50%\n",
    "- This represents the revenue that the company stands to lose if no retention action is taken\n",
    "\n",
    "> We exclude already-churned customers from the RAR calculation since their revenue is already lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ca27c",
   "metadata": {},
   "source": [
    "### 5.1 Churn Probability Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict churn probability for all customers using Random Forest\n",
    "df['Churn_Probability'] = rf.predict_proba(df[feature_cols])[:, 1]\n",
    "\n",
    "# Sanity check: mean probability should be high for actual churners, low for non-churners\n",
    "means = df.groupby('Churn')['Churn_Probability'].mean()\n",
    "print(\"Mean churn probability by actual status:\")\n",
    "print(f\"  Non-churned (0): {means[0]:.3f}\")\n",
    "print(f\"  Churned     (1): {means[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8344bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability distribution by actual churn status\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.histplot(data=df, x='Churn_Probability', hue='Churn', bins=30, kde=True,\n",
    "             stat='density', common_norm=False, palette={0: 'steelblue', 1: 'coral'}, alpha=0.6, ax=ax)\n",
    "\n",
    "# Mark the means\n",
    "ymax = ax.get_ylim()[1]\n",
    "for cls, color, label in [(0, 'steelblue', 'No Churn'), (1, 'coral', 'Churn')]:\n",
    "    m = means[cls]\n",
    "    ax.axvline(m, color=color, linestyle='--', linewidth=2)\n",
    "    ax.text(m, ymax * 0.92, f'{label} mean: {m:.3f}', color='white',\n",
    "            bbox=dict(facecolor=color, edgecolor='none', alpha=0.85), ha='center', fontsize=10)\n",
    "\n",
    "ax.set_title('Churn Probability Distribution by Actual Churn Status')\n",
    "ax.set_xlabel('Predicted Churn Probability')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ecda1",
   "metadata": {},
   "source": [
    "### 5.2 Revenue at Risk Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sjm4ljhps5i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualize monthly charges\n",
    "df['AnnualCharges'] = df['MonthlyCharges'] * 12\n",
    "\n",
    "# Filter: non-churned customers with predicted churn probability > 50%\n",
    "at_risk = df[(df['Churn_Probability'] > 0.5) & (df['Churn'] == 0)]\n",
    "total_non_churned_revenue = df[df['Churn'] == 0]['AnnualCharges'].sum()\n",
    "\n",
    "revenue_at_risk = at_risk['AnnualCharges'].sum()\n",
    "rar_pct = revenue_at_risk / total_non_churned_revenue * 100\n",
    "\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  REVENUE AT RISK SUMMARY\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  At-risk customers:        {len(at_risk):,}\")\n",
    "print(f\"  Revenue at Risk:           ${revenue_at_risk:,.2f} / year\")\n",
    "print(f\"  Total non-churned revenue: ${total_non_churned_revenue:,.2f} / year\")\n",
    "print(f\"  RAR as % of revenue:       {rar_pct:.2f}%\")\n",
    "print(f\"{'='*55}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88jpwnr3x0m",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Conclusions\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "1. **Class imbalance:** Only 26.5% of customers churned. SMOTE oversampling and class weighting were used to prevent models from defaulting to the majority class.\n",
    "\n",
    "2. **Customer segmentation:** K-Means clustering (k=3) revealed that **Cluster 0 concentrates ~75% of all churned customers** with a ~49.5% churn rate, compared to ~15-18% in the other clusters.\n",
    "\n",
    "3. **Top churn drivers** (by permutation importance and SHAP):\n",
    "   - **Number of tech support tickets** — by far the strongest predictor\n",
    "   - **No internet service** — customers without internet churn less\n",
    "   - **Two-year contracts** — strong protective effect against churn\n",
    "   - **Monthly charges** — higher charges correlate with higher churn risk\n",
    "\n",
    "4. **Revenue at Risk:** The Random Forest model identifies **969 currently non-churned customers** with >50% predicted churn probability, representing **$743,722 in annual revenue at risk** (19.55% of total non-churned revenue).\n",
    "\n",
    "## Business Implications\n",
    "\n",
    "- **Proactive retention:** Target the 969 at-risk customers with retention offers (discounts, contract upgrades, loyalty programs)\n",
    "- **Improve tech support:** The number of tech tickets is the dominant churn signal — improving service quality and first-contact resolution could directly reduce churn\n",
    "- **Promote long-term contracts:** Two-year contracts significantly reduce churn risk; incentivize month-to-month customers to switch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
