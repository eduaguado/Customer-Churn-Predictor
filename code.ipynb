{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a98c30e1",
   "metadata": {},
   "outputs": [],
   "source": "# Customer Churn Prediction & Revenue at Risk Analysis\n\n**Course:** Quantitative Risk Management — Final Project  \n**Author:** Eduard Aguado Marin  \n\n## Objective\n\nThis notebook builds a machine learning pipeline to predict customer churn in a telecom company and quantify the **Revenue at Risk (RAR)** — the annual revenue exposed to loss from customers likely to leave.\n\n**Pipeline overview:**\n1. **Data Cleaning & Feature Engineering** — parse European-format monetary columns, bucketize tenure, encode categoricals\n2. **Exploratory Data Analysis** — distributions, correlations, PCA dimensionality reduction, K-Means clustering\n3. **Modelling** — Logistic Regression (baseline), Random Forest, XGBoost with GridSearchCV; SMOTE oversampling to handle class imbalance\n4. **Model Interpretation** — SHAP values, permutation importance, ROC curves\n5. **Revenue at Risk** — translate churn probabilities into dollar-denominated business risk"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36549883",
   "metadata": {},
   "outputs": [],
   "source": "# ── Imports ──────────────────────────────────────────────────────────────────\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\nimport xgboost as xgb\n\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import (\n    silhouette_score, confusion_matrix, classification_report,\n    roc_auc_score, roc_curve\n)\nfrom sklearn.model_selection import (\n    train_test_split, GridSearchCV, StratifiedKFold\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\n# Plotting defaults\nsns.set_style('whitegrid')\nplt.rcParams['figure.dpi'] = 100"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4d5a2ea2",
   "metadata": {},
   "outputs": [],
   "source": "---\n# 1. Data Loading & Initial Inspection\n\nThe dataset contains **7,043 telecom customer records** with 23 columns covering demographics, service subscriptions, billing, support tickets, and the binary churn label.\n\n> **Note:** The CSV uses a semicolon separator and European decimal format (commas instead of dots), so `MonthlyCharges` and `TotalCharges` are initially read as strings."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f29ff",
   "metadata": {},
   "outputs": [],
   "source": "df = pd.read_csv('telcom_dataset.csv', sep=';')\ndf.replace('', np.nan, inplace=True)\n\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nMissing values:\\n{df.isna().sum()[df.isna().sum() > 0]}\"\n      if df.isna().any().any() else \"No missing values found.\")\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7459122",
   "metadata": {},
   "outputs": [],
   "source": "# Unique values per column — helps decide encoding strategy\n# Binary (2 unique) → 0/1, Few categories (3-4) → one-hot, Continuous → keep as-is\ndf.nunique()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72139ce3",
   "metadata": {},
   "outputs": [],
   "source": "# MonthlyCharges and TotalCharges are stored as object (string) due to European comma decimals\ndf.dtypes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb1a54",
   "metadata": {},
   "outputs": [],
   "source": "# Target variable distribution — 26.5% churn rate indicates moderate class imbalance\ndf.Churn.value_counts(normalize=True)"
  },
  {
   "cell_type": "markdown",
   "id": "46241809",
   "metadata": {},
   "source": "---\n# 2. Data Cleaning & Feature Engineering\n\nSteps:\n1. Drop `customerID` (not predictive)\n2. Parse `MonthlyCharges` / `TotalCharges` from European string format → float\n3. Bucketize `tenure` into 7 interpretable groups (0-6 mo, 6-12 mo, ..., 60+ mo)\n4. Encode binary columns (Yes/No → 1/0) and one-hot encode multi-category columns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cd34c",
   "metadata": {},
   "outputs": [],
   "source": "# Drop customerID — it's a unique identifier, not a feature\nif 'customerID' in df.columns:\n    df.drop('customerID', axis=1, inplace=True)\n\ndef clean_monetary_columns(df, cols=('TotalCharges', 'MonthlyCharges')):\n    \"\"\"Convert European-format monetary strings to float.\n    \n    Handles: '1.234,56' (dot thousands, comma decimal), '12,5' (comma decimal only),\n    empty strings, and NaN values.\n    \"\"\"\n    for col in cols:\n        if col not in df.columns:\n            continue\n        s = df[col].astype(str).str.strip()\n        s = s.mask((s == '') | (s.str.lower() == 'nan'), '0')\n\n        has_comma = s.str.contains(',', regex=False)\n        has_dot = s.str.contains('.', regex=False)\n        s2 = s.copy()\n        mask_both = has_comma & has_dot\n\n        # '1.234,56' → remove dot thousands separator, then comma → dot\n        if mask_both.any():\n            s2.loc[mask_both] = (\n                s.loc[mask_both]\n                .str.replace('.', '', regex=False)\n                .str.replace(',', '.', regex=False)\n            )\n\n        # '12,5' → '12.5'\n        only_comma = has_comma & ~mask_both\n        if only_comma.any():\n            s2.loc[only_comma] = s.loc[only_comma].str.replace(',', '.', regex=False)\n\n        s2 = s2.str.replace(r'[^0-9\\.\\-]', '', regex=True)\n        df[col] = pd.to_numeric(s2, errors='coerce').fillna(0.0)\n\n    return df\n\ndf = clean_monetary_columns(df)\nprint(f\"MonthlyCharges dtype: {df['MonthlyCharges'].dtype}, range: [{df['MonthlyCharges'].min():.2f}, {df['MonthlyCharges'].max():.2f}]\")\nprint(f\"TotalCharges   dtype: {df['TotalCharges'].dtype},   range: [{df['TotalCharges'].min():.2f}, {df['TotalCharges'].max():.2f}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf628c",
   "metadata": {},
   "outputs": [],
   "source": "# Bucketize tenure into interpretable groups\n# Raw tenure has 73 unique values (months) — grouping reduces noise and aids interpretation\nTENURE_LABELS = {1: '0–6 mo', 2: '6–12 mo', 3: '12–24 mo',\n                 4: '24–36 mo', 5: '36–48 mo', 6: '48–60 mo', 7: '60+ mo'}\n\ndef bucketize_tenure(t):\n    if t <= 6:   return 1\n    if t <= 12:  return 2\n    if t <= 24:  return 3\n    if t <= 36:  return 4\n    if t <= 48:  return 5\n    if t <= 60:  return 6\n    return 7\n\ndf['tenure_group'] = df['tenure'].apply(bucketize_tenure)\ndf.drop('tenure', axis=1, inplace=True)\n\nprint(\"Tenure group distribution:\")\nprint(df['tenure_group'].value_counts().sort_index().rename(TENURE_LABELS))"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "7eed941c",
   "metadata": {},
   "outputs": [],
   "source": "### 2.1 Feature Encoding\n\nEncoding strategy:\n- **Binary Yes/No columns** → 0/1 (including those with \"No internet service\" / \"No phone service\" mapped to 0)\n- **Gender** → binary `male` column (Female=0, Male=1)\n- **Multi-category columns** (`InternetService`, `Contract`, `PaymentMethod`) → one-hot encoding with `drop_first=True` to avoid multicollinearity"
  },
  {
   "cell_type": "code",
   "id": "d9f9b130",
   "metadata": {},
   "source": "# Gender → binary\ndf['male'] = df['gender'].map({'Female': 0, 'Male': 1})\ndf.drop('gender', axis=1, inplace=True)\n\n# Yes/No columns → 0/1\nfor col in ['Partner', 'Dependents', 'PaperlessBilling', 'Churn', 'PhoneService']:\n    df[col] = df[col].map({'No': 0, 'Yes': 1})\n\n# Columns with \"No internet service\" → treat as 0 (service not used)\nfor col in ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n            'TechSupport', 'StreamingTV', 'StreamingMovies']:\n    df[col] = df[col].map({'No internet service': 0, 'No': 0, 'Yes': 1})\n\n# MultipleLines: \"No phone service\" → 0\ndf['MultipleLines'] = df['MultipleLines'].map({'No phone service': 0, 'No': 0, 'Yes': 1})\n\n# One-hot encode multi-category features (drop_first avoids perfect multicollinearity)\ndf = pd.get_dummies(df, columns=['InternetService', 'Contract', 'PaymentMethod'], drop_first=True)\ndf = df.astype({col: int for col in df.select_dtypes('bool').columns})\n\n# Sanitize column names (replace spaces with underscores)\ndf.columns = df.columns.str.replace(' ', '_')\n\nprint(f\"Final shape: {df.shape}\")\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd9bff",
   "metadata": {},
   "outputs": [],
   "source": "# Final class distribution after encoding\nchurn_counts = df['Churn'].value_counts()\nprint(f\"No Churn: {churn_counts[0]} ({churn_counts[0]/len(df)*100:.1f}%)\")\nprint(f\"Churn:    {churn_counts[1]} ({churn_counts[1]/len(df)*100:.1f}%)\")\nprint(f\"\\nTotal features: {df.shape[1] - 1} + 1 target = {df.shape[1]} columns\")"
  },
  {
   "cell_type": "markdown",
   "id": "a833c93a",
   "metadata": {},
   "source": "---\n# 3. Exploratory Data Analysis\n\nWe examine the data from three angles:\n1. **Univariate distributions** — understand the shape of each feature\n2. **Feature correlations** — identify multicollinearity and features associated with churn\n3. **PCA + K-Means clustering** — discover natural customer segments and their churn concentration"
  },
  {
   "cell_type": "markdown",
   "id": "de7c496e",
   "metadata": {},
   "source": "### 3.1 Target Variable Distribution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98d669",
   "metadata": {},
   "outputs": [],
   "source": "# ── Helper: plot distributions for a set of columns ──\ndef plot_variable_distributions(df, title_prefix=''):\n    num_vars = df.select_dtypes(include=['int64', 'float64']).columns\n    cat_vars = df.select_dtypes(include=['object', 'category']).columns\n    total_vars = len(num_vars) + len(cat_vars)\n    cols = 3\n    rows = (total_vars + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n    axes = axes.flatten()\n\n    for i, col in enumerate(num_vars):\n        if set(df[col].dropna().unique()).issubset({0, 1}):\n            sns.countplot(x=df[col], ax=axes[i])\n            axes[i].set_xticks([0, 1])\n        else:\n            sns.histplot(df[col], bins=20, kde=False, ax=axes[i])\n        axes[i].set_title(f'{title_prefix}{col}')\n\n    for j, col in enumerate(cat_vars, start=len(num_vars)):\n        sns.countplot(x=df[col], ax=axes[j])\n        axes[j].set_title(f'{title_prefix}{col}')\n        axes[j].tick_params(axis='x', rotation=45)\n\n    for k in range(total_vars, len(axes)):\n        fig.delaxes(axes[k])\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb506",
   "metadata": {},
   "outputs": [],
   "source": "# Churn vs Non-Churn bar plot with counts and percentages\nchurn_counts = df['Churn'].value_counts()\nchurn_percent = df['Churn'].value_counts(normalize=True) * 100\n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.barplot(x=churn_counts.index, y=churn_counts.values, palette='viridis', ax=ax)\nfor i, (count, pct) in enumerate(zip(churn_counts.values, churn_percent.values)):\n    ax.text(i, count + 50, f'{count} ({pct:.1f}%)', ha='center', va='bottom', fontsize=12)\nax.set_xticklabels(['No Churn', 'Churn'])\nax.set_ylabel('Number of Customers')\nax.set_title('Customer Churn Distribution')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4c9b83c7",
   "metadata": {},
   "outputs": [],
   "source": "### 3.2 Feature Distributions\n\nWe split features into **binary** (0/1) and **non-binary** groups for cleaner visualization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15d1fe",
   "metadata": {},
   "outputs": [],
   "source": "# Binary features (0/1)\nbinary_vars = [col for col in df.columns if set(df[col].dropna().unique()).issubset({0, 1})]\nplot_variable_distributions(df[binary_vars])"
  },
  {
   "cell_type": "code",
   "id": "55796553",
   "metadata": {},
   "source": "# Non-binary features (continuous and multi-category)\nother_vars = [col for col in df.columns if col not in binary_vars]\nplot_variable_distributions(df[other_vars])"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9b1b1877",
   "metadata": {},
   "outputs": [],
   "source": "### 3.3 Feature Correlations\n\nThe heatmap below highlights which features are most correlated with `Churn` and with each other. Strong inter-feature correlations (e.g., `MonthlyCharges` ↔ streaming services) may affect logistic regression coefficients but are handled well by tree-based models."
  },
  {
   "cell_type": "code",
   "id": "6d825218",
   "metadata": {},
   "source": "plt.figure(figsize=(14, 11))\nsns.heatmap(df.corr(numeric_only=True), annot=True, fmt='.1f', cmap='coolwarm', square=True,\n            linewidths=0.5, cbar_kws={'shrink': 0.8})\nplt.title('Feature Correlation Heatmap')\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "38d00f8a",
   "metadata": {},
   "outputs": [],
   "source": "### 3.4 PCA & K-Means Clustering\n\n**Goal:** Identify natural customer segments and check whether churn concentrates in specific clusters.\n\n**Approach:**\n- Standardize features, then apply PCA retaining 95% of variance\n- Use the **elbow method** (WCSS) and **silhouette scores** to evaluate cluster quality\n- Visualize clusters in the first two principal components, split by churn status\n\n> **Note on k selection:** The silhouette score is maximized at k=2, but we choose **k=3** because it produces more interpretable segments — the 3-cluster solution clearly separates a high-churn group (Cluster 0) from two lower-churn groups, whereas k=2 merges these distinct behavioral profiles into a single heterogeneous cluster."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d86d4",
   "metadata": {},
   "outputs": [],
   "source": "# ── PCA ──────────────────────────────────────────────────────────────────────\nX = df.drop(columns='Churn')\ny = df['Churn']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\npca_full = PCA(n_components=0.95)\nX_pca = pca_full.fit_transform(X_scaled)\nprint(f\"PCA retained {X_pca.shape[1]} components, explaining \"\n      f\"{pca_full.explained_variance_ratio_.sum():.1%} of variance\")\n\n# ── Evaluate k with silhouette scores ────────────────────────────────────────\nk_range = range(2, 11)\nsil_scores = []\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = km.fit_predict(X_pca)\n    sil_scores.append(silhouette_score(X_pca, labels))\n\nprint(\"\\nSilhouette scores:\", dict(zip(k_range, [round(s, 3) for s in sil_scores])))\nprint(f\"Best k by silhouette: {list(k_range)[sil_scores.index(max(sil_scores))]}\")\n\n# We override the silhouette-optimal k with k=3 for better interpretability\n# (see markdown note above — k=3 separates the high-churn group more clearly)\nCHOSEN_K = 3\nprint(f\"Selected k = {CHOSEN_K}\")\n\n# ── Elbow method (WCSS) ─────────────────────────────────────────────────────\nk_values = range(1, 16)\nwcss = []\nfor k in k_values:\n    km = KMeans(n_clusters=k, random_state=42, n_init=20)\n    km.fit(X_pca)\n    wcss.append(km.inertia_)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(list(k_values), wcss, marker='o', linewidth=2)\naxes[0].axvline(CHOSEN_K, color='red', linestyle='--', alpha=0.7, label=f'Chosen k={CHOSEN_K}')\naxes[0].set_xlabel('Number of clusters k')\naxes[0].set_ylabel('WCSS (inertia)')\naxes[0].set_title('Elbow Method')\naxes[0].set_xticks(list(k_values))\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\naxes[1].plot(list(k_range), sil_scores, marker='s', linewidth=2, color='orange')\naxes[1].axvline(CHOSEN_K, color='red', linestyle='--', alpha=0.7, label=f'Chosen k={CHOSEN_K}')\naxes[1].set_xlabel('Number of clusters k')\naxes[1].set_ylabel('Silhouette Score')\naxes[1].set_title('Silhouette Analysis')\naxes[1].set_xticks(list(k_range))\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcde6f8",
   "metadata": {},
   "outputs": [],
   "source": "# ── Fit K-Means with k=3 and visualize ───────────────────────────────────────\nkmeans = KMeans(n_clusters=CHOSEN_K, random_state=42, n_init=20)\nclusters = kmeans.fit_predict(X_pca)\n\nX_plot = X_pca[:, :2]\nplot_df = pd.DataFrame(X_plot, columns=['PC1', 'PC2']).assign(cluster=clusters, churn=y.values)\n\n# Side-by-side scatter plots: Non-Churned vs Churned\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nsns.scatterplot(data=plot_df[plot_df['churn'] == 0], x='PC1', y='PC2', hue='cluster',\n                s=50, alpha=0.8, edgecolor='k', linewidth=0.3, ax=axes[0])\naxes[0].set_title('Non-Churned Customers')\n\nsns.scatterplot(data=plot_df[plot_df['churn'] == 1], x='PC1', y='PC2', hue='cluster',\n                s=50, alpha=0.8, edgecolor='k', linewidth=0.3, ax=axes[1])\naxes[1].set_title('Churned Customers')\n\nplt.tight_layout()\nplt.show()\n\n# Churn distribution per cluster\nprint(\"\\nCluster sizes and churn rates:\")\nct = pd.crosstab(plot_df['cluster'], plot_df['churn'], margins=True)\nct.columns = ['No Churn', 'Churn', 'Total']\nct['Churn Rate'] = (ct['Churn'] / ct['Total'] * 100).round(1).astype(str) + '%'\nprint(ct)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57432ef9",
   "metadata": {},
   "outputs": [],
   "source": "# PCA loadings — which original features contribute most to PC1 and PC2\npc_df = pd.DataFrame(\n    pca_full.components_[:2].T,\n    index=X.columns,\n    columns=['PC1', 'PC2']\n).sort_values('PC1', ascending=False)\n\nprint(\"Top PC1 drivers: MonthlyCharges, TotalCharges, StreamingTV/Movies (service engagement)\")\nprint(\"Top PC2 drivers: Contract_Two_year, tenure_group (customer loyalty) vs Electronic check, Fiber optic (churn risk)\")\npc_df"
  },
  {
   "cell_type": "markdown",
   "id": "4211de28",
   "metadata": {},
   "source": "**Key finding:** ~75% of all churned customers are concentrated in **Cluster 0** (1,272 out of 1,869). This cluster has a ~49.5% churn rate, compared to ~15-18% in the other clusters. Feature importance analysis in the modelling section will help explain *why* these customers churn — likely driven by short tenure, fiber optic internet, month-to-month contracts, and high tech ticket counts."
  },
  {
   "cell_type": "markdown",
   "id": "7afeef71",
   "metadata": {},
   "source": "---\n# 4. Modelling\n\nWe train three classifiers of increasing complexity:\n\n| Model | Purpose |\n|-------|---------|\n| **Logistic Regression** | Interpretable linear baseline |\n| **Random Forest** | Non-linear ensemble with built-in feature importance |\n| **XGBoost** | Gradient-boosted trees, hyperparameter-tuned via GridSearchCV |\n\n**Class imbalance handling:**\n- **SMOTE** (Synthetic Minority Over-sampling Technique) is applied to the **training set only** to avoid data leakage into the test set\n- Random Forest also uses `class_weight={0: 1, 1: 2}` as additional imbalance correction"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "03e2fae6",
   "metadata": {},
   "outputs": [],
   "source": "### 4.1 Train/Test Split & Helper Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f2352",
   "metadata": {},
   "outputs": [],
   "source": "# ── Train/test split (80/20, stratified on Churn) ────────────────────────────\nfeature_cols = df.columns[df.columns != 'Churn']\ntrain, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Churn'])\n\nX_train = train[feature_cols]\nY_train = train['Churn']\nX_test  = test[feature_cols]\nY_test  = test['Churn']\n\nprint(f\"Train: {X_train.shape[0]} samples | Test: {X_test.shape[0]} samples\")\nprint(f\"Train churn rate: {Y_train.mean():.1%} | Test churn rate: {Y_test.mean():.1%}\")\n\n# ── Helper functions ─────────────────────────────────────────────────────────\ndef print_metrics(model, X_train, Y_train, X_test, Y_test, label=\"Model\"):\n    \"\"\"Print classification reports for train and test sets.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"  {label} — Classification Report\")\n    print(f\"{'='*60}\")\n    print(\"\\nTraining Set:\")\n    print(classification_report(Y_train, model.predict(X_train)))\n    print(\"Test Set:\")\n    print(classification_report(Y_test, model.predict(X_test)))\n\ndef plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n    \"\"\"Plot a heatmap confusion matrix.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n    plt.title(title)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "11f1de60",
   "metadata": {},
   "source": "### 4.2 SMOTE Oversampling\n\nSMOTE generates synthetic minority-class samples by interpolating between existing minority neighbors. It is applied **only to the training set** — the test set retains the original class distribution to give realistic performance estimates."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba41bb8",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Before SMOTE:\", Counter(Y_train))\nsmote = SMOTE(random_state=42)\nX_train, Y_train = smote.fit_resample(X_train, Y_train)\nprint(\"After  SMOTE:\", Counter(Y_train))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2529b50",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize class balance after SMOTE (training) vs original (test)\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor ax, data, title in [(axes[0], Y_train, 'Training Set (After SMOTE)'),\n                         (axes[1], Y_test, 'Test Set (Original Distribution)')]:\n    counts = data.value_counts().sort_index()\n    pcts = (counts / counts.sum() * 100)\n    sns.barplot(x=counts.index, y=counts.values, palette='viridis', ax=ax)\n    for i, (cnt, pct) in enumerate(zip(counts.values, pcts.values)):\n        ax.text(i, cnt + cnt * 0.02, f'{cnt} ({pct:.1f}%)', ha='center', fontsize=11)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['No Churn', 'Churn'])\n    ax.set_ylabel('Count')\n    ax.set_title(title)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "b891b481",
   "metadata": {},
   "source": "### 4.3 Model 1: Logistic Regression (Baseline)\n\nA simple linear model that serves as our baseline. Since SMOTE already balances the training data, we use default settings with `max_iter=1000` to ensure convergence."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f9f8f",
   "metadata": {},
   "outputs": [],
   "source": "lr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, Y_train)\n\nprint_metrics(lr, X_train, Y_train, X_test, Y_test, label=\"Logistic Regression\")\nplot_confusion_matrix(Y_test, lr.predict(X_test), title=\"Logistic Regression — Confusion Matrix (Test Set)\")"
  },
  {
   "cell_type": "markdown",
   "id": "360c32b1",
   "metadata": {},
   "source": "### 4.4 Model 2: Random Forest\n\nKey hyperparameters:\n- `n_estimators=1000` — large ensemble for stable predictions\n- `criterion='entropy'` — information gain split criterion\n- `max_depth=10` — limits tree depth to reduce overfitting\n- `class_weight={0: 1, 1: 2}` — additional penalty for misclassifying churned customers (on top of SMOTE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa55ee",
   "metadata": {},
   "outputs": [],
   "source": "rf = RandomForestClassifier(\n    n_estimators=1000, criterion='entropy', max_depth=10,\n    max_features='sqrt', min_samples_split=8,\n    class_weight={0: 1, 1: 2}, random_state=42, n_jobs=-1\n)\nrf.fit(X_train, Y_train)\n\nprint_metrics(rf, X_train, Y_train, X_test, Y_test, label=\"Random Forest\")\nplot_confusion_matrix(Y_test, rf.predict(X_test), title=\"Random Forest — Confusion Matrix (Test Set)\")\n\n# Gini-based feature importance\nimportances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nprint(\"\\nTop 10 features (Gini importance):\")\nprint(importances.head(10).to_string())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b36dc1",
   "metadata": {},
   "outputs": [],
   "source": "# Permutation importance — more reliable than Gini for correlated features\n# Measures actual accuracy drop when each feature is randomly shuffled\nperm_result = permutation_importance(rf, X_test, Y_test, n_repeats=30, random_state=42, n_jobs=-1)\nperm_importances = pd.Series(perm_result.importances_mean, index=X_test.columns).sort_values(ascending=False)\n\nprint(\"Top 10 features (permutation importance):\")\nprint(perm_importances.head(10).to_string())"
  },
  {
   "cell_type": "markdown",
   "id": "44cdfd7b",
   "metadata": {},
   "source": "### 4.5 Model 3: XGBoost (with GridSearchCV)\n\nXGBoost is tuned via exhaustive grid search over key hyperparameters, using **5-fold stratified cross-validation** and **ROC-AUC** as the scoring metric (better suited for imbalanced targets than accuracy)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606e96a",
   "metadata": {},
   "outputs": [],
   "source": "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nparam_grid_xgb = {\n    'n_estimators':      [200, 500, 1000],\n    'max_depth':         [3, 6, 10],\n    'learning_rate':     [0.01, 0.1, 0.2],\n    'subsample':         [0.7, 1.0],\n    'colsample_bytree':  [0.7, 1.0],\n    'scale_pos_weight':  [1, 2],\n}\n\ngs_xgb = GridSearchCV(\n    xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss'),\n    param_grid_xgb,\n    scoring='roc_auc',\n    cv=cv,\n    n_jobs=-1,\n    verbose=1\n)\ngs_xgb.fit(X_train, Y_train)\n\nprint(f\"\\nBest CV ROC-AUC: {gs_xgb.best_score_:.4f}\")\nprint(f\"Best parameters: {gs_xgb.best_params_}\")\n\nxgb_model = gs_xgb.best_estimator_\nprint_metrics(xgb_model, X_train, Y_train, X_test, Y_test, label=\"XGBoost\")\nplot_confusion_matrix(Y_test, xgb_model.predict(X_test), title=\"XGBoost — Confusion Matrix (Test Set)\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "210a07b3",
   "metadata": {},
   "outputs": [],
   "source": "### 4.6 Model Comparison — ROC Curves\n\nThe ROC curve plots the True Positive Rate vs False Positive Rate at various classification thresholds. A higher AUC (Area Under Curve) indicates better discrimination between churned and non-churned customers."
  },
  {
   "cell_type": "code",
   "id": "493f7e68",
   "metadata": {},
   "source": "models = {\n    \"Logistic Regression\": lr,\n    \"Random Forest\": rf,\n    \"XGBoost\": xgb_model\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nfor ax, (label, model) in zip(axes, models.items()):\n    y_proba = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, _ = roc_curve(Y_test, y_proba)\n    auc = roc_auc_score(Y_test, y_proba)\n    ax.plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n    ax.set_title(f'ROC Curve — {label}')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend(loc='lower right')\n    ax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "351f4a2c",
   "metadata": {},
   "outputs": [],
   "source": "### 4.7 SHAP Values (Model Interpretability)\n\nSHAP (SHapley Additive exPlanations) assigns each feature a contribution to each individual prediction:\n- **Bar plot** — global feature importance (mean |SHAP| across all test samples)\n- **Beeswarm plot** — shows both importance *and* direction of effect (red = high feature value, blue = low)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29df1f2",
   "metadata": {},
   "outputs": [],
   "source": "# Compute SHAP values for the Random Forest model\nexplainer = shap.Explainer(rf)\nshap_values = explainer(X_test)\n\n# Global feature importance\nshap.plots.bar(shap_values, max_display=15, show=False)\nplt.title('SHAP — Global Feature Importance (Random Forest)')\nplt.tight_layout()\nplt.show()\n\n# Beeswarm: direction + magnitude of feature effects\nshap.plots.beeswarm(shap_values, max_display=10, show=False)\nplt.title('SHAP — Beeswarm Plot (Random Forest)')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "180b606b",
   "metadata": {},
   "source": "---\n# 5. Revenue at Risk (RAR) Analysis\n\nThe final step translates model predictions into **business value**. We use the Random Forest model to estimate each customer's churn probability, then calculate:\n\n- **Revenue at Risk** = sum of annualized charges for *currently non-churned* customers whose predicted churn probability exceeds 50%\n- This represents the revenue that the company stands to lose if no retention action is taken\n\n> We exclude already-churned customers from the RAR calculation since their revenue is already lost."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "bf3ca27c",
   "metadata": {},
   "outputs": [],
   "source": "### 5.1 Churn Probability Distribution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7b42f",
   "metadata": {},
   "outputs": [],
   "source": "# Predict churn probability for all customers using Random Forest\ndf['Churn_Probability'] = rf.predict_proba(df[feature_cols])[:, 1]\n\n# Sanity check: mean probability should be high for actual churners, low for non-churners\nmeans = df.groupby('Churn')['Churn_Probability'].mean()\nprint(\"Mean churn probability by actual status:\")\nprint(f\"  Non-churned (0): {means[0]:.3f}\")\nprint(f\"  Churned     (1): {means[1]:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8344bdc",
   "metadata": {},
   "outputs": [],
   "source": "# Probability distribution by actual churn status\nfig, ax = plt.subplots(figsize=(9, 6))\nsns.histplot(data=df, x='Churn_Probability', hue='Churn', bins=30, kde=True,\n             stat='density', common_norm=False, palette={0: 'steelblue', 1: 'coral'}, alpha=0.6, ax=ax)\n\n# Mark the means\nymax = ax.get_ylim()[1]\nfor cls, color, label in [(0, 'steelblue', 'No Churn'), (1, 'coral', 'Churn')]:\n    m = means[cls]\n    ax.axvline(m, color=color, linestyle='--', linewidth=2)\n    ax.text(m, ymax * 0.92, f'{label} mean: {m:.3f}', color='white',\n            bbox=dict(facecolor=color, edgecolor='none', alpha=0.85), ha='center', fontsize=10)\n\nax.set_title('Churn Probability Distribution by Actual Churn Status')\nax.set_xlabel('Predicted Churn Probability')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "1a2ecda1",
   "metadata": {},
   "outputs": [],
   "source": "### 5.2 Revenue at Risk Calculation"
  },
  {
   "cell_type": "code",
   "id": "sjm4ljhps5i",
   "source": "# Annualize monthly charges\ndf['AnnualCharges'] = df['MonthlyCharges'] * 12\n\n# Filter: non-churned customers with predicted churn probability > 50%\nat_risk = df[(df['Churn_Probability'] > 0.5) & (df['Churn'] == 0)]\ntotal_non_churned_revenue = df[df['Churn'] == 0]['AnnualCharges'].sum()\n\nrevenue_at_risk = at_risk['AnnualCharges'].sum()\nrar_pct = revenue_at_risk / total_non_churned_revenue * 100\n\nprint(f\"{'='*55}\")\nprint(f\"  REVENUE AT RISK SUMMARY\")\nprint(f\"{'='*55}\")\nprint(f\"  At-risk customers:        {len(at_risk):,}\")\nprint(f\"  Revenue at Risk:           ${revenue_at_risk:,.2f} / year\")\nprint(f\"  Total non-churned revenue: ${total_non_churned_revenue:,.2f} / year\")\nprint(f\"  RAR as % of revenue:       {rar_pct:.2f}%\")\nprint(f\"{'='*55}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "88jpwnr3x0m",
   "source": "---\n# 6. Conclusions\n\n## Key Findings\n\n1. **Class imbalance:** Only 26.5% of customers churned. SMOTE oversampling and class weighting were used to prevent models from defaulting to the majority class.\n\n2. **Customer segmentation:** K-Means clustering (k=3) revealed that **Cluster 0 concentrates ~75% of all churned customers** with a ~49.5% churn rate, compared to ~15-18% in the other clusters.\n\n3. **Top churn drivers** (by permutation importance and SHAP):\n   - **Number of tech support tickets** — by far the strongest predictor\n   - **No internet service** — customers without internet churn less\n   - **Two-year contracts** — strong protective effect against churn\n   - **Monthly charges** — higher charges correlate with higher churn risk\n\n4. **Revenue at Risk:** The Random Forest model identifies **969 currently non-churned customers** with >50% predicted churn probability, representing **$743,722 in annual revenue at risk** (19.55% of total non-churned revenue).\n\n## Business Implications\n\n- **Proactive retention:** Target the 969 at-risk customers with retention offers (discounts, contract upgrades, loyalty programs)\n- **Improve tech support:** The number of tech tickets is the dominant churn signal — improving service quality and first-contact resolution could directly reduce churn\n- **Promote long-term contracts:** Two-year contracts significantly reduce churn risk; incentivize month-to-month customers to switch",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}